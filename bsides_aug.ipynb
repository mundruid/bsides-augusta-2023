{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sweetviz as sv\n",
    "import ipaddress\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pandasai.llm.openai import OpenAI\n",
    "from pandasai import PandasAI\n",
    "from scapy.all import PcapReader, IP, TCP, UDP, ICMP\n",
    "from scipy.stats import ttest_ind, kstest, norm, skew, kurtosis, zscore\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from skimpy import skim\n",
    "from summarytools import dfSummary\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds .env file and loads the vars\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\", \"Key not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcap_reader_mirai = PcapReader(\"data/mirai.pcap\")\n",
    "pcap_reader_benign = PcapReader(\"data/benign.pcapng\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess\n",
    "\n",
    "- convert data to streams\n",
    "- collect some numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcap_to_dataframe(pcap_reader: PcapReader) -> pd.DataFrame:\n",
    "    \"\"\"Converts a packet capture to a Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        pcap_reader (PcapReader): packet capture read using scapy\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: dataframe with pcap data\n",
    "    \"\"\"\n",
    "    # Create an empty list to store the data\n",
    "    data = []\n",
    "\n",
    "    # Iterate through the packets in the pcap file\n",
    "    for packet in pcap_reader:\n",
    "        # Get the source and destination IP addresses\n",
    "        if packet.haslayer(IP):\n",
    "            src_ip = packet[IP].src\n",
    "            dst_ip = packet[IP].dst\n",
    "            protocol = packet[IP].proto\n",
    "        else:\n",
    "            src_ip = None\n",
    "            dst_ip = None\n",
    "            protocol = None\n",
    "\n",
    "        # Get the source and destination ports and payload\n",
    "        if packet.haslayer(TCP):\n",
    "            src_port = packet[TCP].sport\n",
    "            dst_port = packet[TCP].dport\n",
    "            payload = str(packet[TCP].payload)\n",
    "            packet_len = len(packet[TCP])\n",
    "        elif packet.haslayer(UDP):\n",
    "            src_port = packet[UDP].sport\n",
    "            dst_port = packet[UDP].dport\n",
    "            payload = str(packet[UDP].payload)\n",
    "            packet_len = len(packet[UDP])\n",
    "        elif packet.haslayer(ICMP):\n",
    "            payload = str(packet[ICMP].payload)\n",
    "            packet_len = len(packet[ICMP])\n",
    "            src_port = None\n",
    "            dst_port = None\n",
    "        else:\n",
    "            src_port = None\n",
    "            dst_port = None\n",
    "            payload = str(packet.payload)\n",
    "            packet_len = len(packet)\n",
    "\n",
    "        # Append the data to the list\n",
    "        data.append(\n",
    "            [\n",
    "                packet.time,\n",
    "                src_ip,\n",
    "                dst_ip,\n",
    "                src_port,\n",
    "                dst_port,\n",
    "                payload,\n",
    "                packet_len,\n",
    "                protocol,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # Convert the list to a pandas dataframe\n",
    "    df = pd.DataFrame(\n",
    "        data,\n",
    "        columns=[\n",
    "            \"Timestamp\",\n",
    "            \"Source IP\",\n",
    "            \"Destination IP\",\n",
    "            \"Source Port\",\n",
    "            \"Destination Port\",\n",
    "            \"Payload\",\n",
    "            \"Packet Length\",\n",
    "            \"Protocol\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mirai_df = pcap_to_dataframe(pcap_reader_mirai)\n",
    "# benign_df = pcap_to_dataframe(pcap_reader_benign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mirai_df.to_pickle(\"../data/bsides_aug/mirai.pkl\")\n",
    "# benign_df.to_pickle(\"../data/bsides_aug/benign.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mirai_df = pd.read_pickle(\"data/mirai.pkl\")\n",
    "benign_df = pd.read_pickle(\"data/benign.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mirai_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI\n",
    "Use pandas AI to clean up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a LLM\n",
    "llm = OpenAI(api_token=openai_api_key)\n",
    "pandas_ai = PandasAI(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mirai_cleaned_df = pandas_ai.clean_data(mirai_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_clean_df = pandas_ai.clean_data(benign_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mirai_cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_clean_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract streams\n",
    "Extract streams using the tuple (src_IP, src_port, dst_ip, dst_port, protocol)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_streams(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Create an empty list to store stream data as separate dataframes\n",
    "    dfs = []\n",
    "\n",
    "    # Group packets by src/dst IP and src/dst port\n",
    "    grouped = df.groupby(\n",
    "        [\"Source IP\", \"Destination IP\", \"Source Port\", \"Destination Port\", \"Protocol\"]\n",
    "    )\n",
    "\n",
    "    # Iterate through each group to extract stream data\n",
    "    for name, group in grouped:\n",
    "        # Get source/destination IP, port, and protocol\n",
    "        src_ip, dst_ip, src_port, dst_port, proto = name\n",
    "\n",
    "        # Get number of packets, total length, and duration of the stream\n",
    "        num_packets = len(group)\n",
    "        total_length = group[\"Packet Length\"].sum()\n",
    "        start_time = group[\"Timestamp\"].min()\n",
    "        end_time = group[\"Timestamp\"].max()\n",
    "        duration = float(end_time - start_time)\n",
    "\n",
    "        # Create a new dataframe with the stream data\n",
    "        stream_df = pd.DataFrame(\n",
    "            {\n",
    "                \"Source IP\": [src_ip],\n",
    "                \"Destination IP\": [dst_ip],\n",
    "                \"Source Port\": [src_port],\n",
    "                \"Destination Port\": [dst_port],\n",
    "                \"Protocol\": [proto],\n",
    "                \"Number of Packets\": [num_packets],\n",
    "                \"Total Length\": [total_length],\n",
    "                \"Duration\": [duration],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Add the new dataframe to the list\n",
    "        dfs.append(stream_df)\n",
    "\n",
    "    # Concatenate all the dataframes in the list into one dataframe\n",
    "    stream_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Return the new dataframe with stream data\n",
    "    return stream_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: find whi timestamp is gone in the cleaned dataframes and use cleaned dataframes here\n",
    "mirai_stream_df = extract_streams(mirai_df)\n",
    "benign_stream_df = extract_streams(benign_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "\n",
    "Exploratory Data Analysis approaches the dataset as a black box that we need to visualize and analyze statistically with the following goals:\n",
    "- get insights about our data\n",
    "- test hypotheses\n",
    "- decide on models and further processing, such as feature engineering.\n",
    "\n",
    "EDA can be performed for benign and malicious data. Here we are looking at EDA only for malicious data, however the same functions can be applied to benign."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive statistics & data\n",
    "\n",
    "- Describe columns and data types\n",
    "- Descriptive statistics\n",
    "  -  count, \n",
    "  -  mean, \n",
    "  -  standard deviation, \n",
    "  -  minimum, \n",
    "  -  25th percentile, \n",
    "  -  median (50th percentile), \n",
    "  -  75th percentile, and \n",
    "  -  maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe, summarize etc.\n",
    "mirai_stream_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mirai_stream_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descriptive statistics\n",
    "mirai_stream_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix for numerical values in dataframe\n",
    "mirai_stream_df.corr()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis testing\n",
    "\n",
    "- Is the difference between two groups or variables statistically significant?\n",
    "- Use t-test to compare means of two groups\n",
    "  - assumes that data follows normal distribution\n",
    "- Types of variables\n",
    "  - dependent: the effect of a phenomenon. For example, how does number of HTTP requests mean that a network is compromised?\n",
    "  - independent: the cause. The number of HTTP requests affects whether a network is compromised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothesis_testing(df, col1, col2):\n",
    "    group1 = df[col1]\n",
    "    group2 = df[col2]\n",
    "    pvalue = ttest_ind(group1, group2)[1]\n",
    "    if pvalue < 0.05:\n",
    "        return \"The difference between {} and {} is statistically significant (p < 0.05)\".format(\n",
    "            col1, col2\n",
    "        )\n",
    "    else:\n",
    "        return \"The difference between {} and {} is not statistically significant (p >= 0.05)\".format(\n",
    "            col1, col2\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis_testing(mirai_stream_df, \"Number of Packets\", \"Total Length\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers\n",
    "\n",
    "- observation that significantly differs from others in a dataset\n",
    "- Causes\n",
    "  - measurement errors\n",
    "  - extreme rare values\n",
    "- significant impact in statistical analysis\n",
    "- measurements\n",
    "  - z-score: `(x - mean) / std_dev`\n",
    "  - IQR method: this method identifies outliers as observations that are below `Q1 - 1.5IQR` or above `Q3 + 1.5IQR`, where Q1 and Q3 are the first and third quartiles, and IQR is the interquartile range (the difference between Q3 and Q1).\n",
    "  - visual inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_zscore(df, column, threshold=3):\n",
    "    zscores = np.abs(zscore(df[column]))\n",
    "    return df[zscores > threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = detect_outliers_zscore(mirai_stream_df, \"Total Length\", threshold=3)\n",
    "print(outliers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI\n",
    "Explore the data with AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_source_IPs = pandas_ai(\n",
    "    mirai_df, prompt=\"Which are the 5 most popular source IP addresses?\"\n",
    ")\n",
    "top_5_source_IPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_dst_ports = pandas_ai(\n",
    "    mirai_df, prompt=\"Which are the 5 most popular destination ports?\"\n",
    ")\n",
    "top_5_dst_ports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_ai.run(\n",
    "    mirai_stream_df,\n",
    "    prompt=\"Plot the scatter plot of stream durations and number of packets.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_ai.run(benign_stream_df, prompt=\"Plot a barplot of top 10 destination ports.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert ip address to numeric values\n",
    "def ip_to_numeric(ip):\n",
    "    ip_obj = ipaddress.ip_interface(ip)\n",
    "    return int(ip_obj.network.network_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert IPs to numeric mirai data\n",
    "mirai_stream_df[\"Source IP Numeric\"] = mirai_stream_df[\"Source IP\"].apply(ip_to_numeric)\n",
    "mirai_stream_df[\"Destination IP Numeric\"] = mirai_stream_df[\"Destination IP\"].apply(\n",
    "    ip_to_numeric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert IPs to numeric benign data\n",
    "benign_stream_df[\"Source IP Numeric\"] = benign_stream_df[\"Source IP\"].apply(\n",
    "    ip_to_numeric\n",
    ")\n",
    "benign_stream_df[\"Destination IP Numeric\"] = benign_stream_df[\"Destination IP\"].apply(\n",
    "    ip_to_numeric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of non numeric columns for IPs\n",
    "mirai_stream_df_numeric = mirai_stream_df.drop(columns=[\"Source IP\", \"Destination IP\"])\n",
    "benign_stream_df_numeric = benign_stream_df.drop(\n",
    "    columns=[\"Source IP\", \"Destination IP\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert duration from object to float\n",
    "mirai_stream_df[\"Duration\"] = mirai_stream_df_numeric[\"Duration\"].astype(float)\n",
    "benign_stream_df[\"Duration\"] = benign_stream_df_numeric[\"Duration\"].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if all data types are numeric now\n",
    "mirai_stream_df_numeric.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI\n",
    "Generate features using `generate_features` from `pandas_ai` on cleaned up data or subset of original due to long requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_ai.generate_features(mirai_cleaned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_ai.generate_features(benign_clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mirai_subset_df = mirai_df.loc[:1000]\n",
    "benign_subset_df = benign_df.loc[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_ai.generate_features(mirai_subset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_ai.generate_features(benign_subset_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summaries & Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skim(mirai_stream_df_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skim(benign_stream_df_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSummary(mirai_stream_df_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSummary(benign_stream_df_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_report = sv.analyze(mirai_stream_df_numeric)\n",
    "my_report.show_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_report = sv.analyze(benign_stream_df_numeric)\n",
    "my_report.show_html()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeling\n",
    "We label and concatenate benign and malicious before one-hot because there are different ports in each dataset and concatenating the two after one hot will not work with different columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add labels, 0 for benign, 1 for malicious\n",
    "mirai_stream_df_numeric[\"Labels\"] = 1\n",
    "benign_stream_df_numeric[\"Labels\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate dataframes\n",
    "data_df = pd.concat(\n",
    "    [mirai_stream_df_numeric, benign_stream_df_numeric], ignore_index=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering (cont.) categorical \n",
    "## One-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(df, feature):\n",
    "    feature_dummies = pd.get_dummies(df[feature], prefix=feature)\n",
    "    return pd.concat([df, feature_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = one_hot_encode(data_df, \"Source Port\")\n",
    "data_df = one_hot_encode(data_df, \"Destination Port\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "- models\n",
    "  - xgboost\n",
    "  - NN\n",
    "  - k-NN\n",
    "  - Random Forest\n",
    "- k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "kfold = KFold(n_splits=k, shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomize the data. TODO: is this needed\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "data_df = shuffle(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after feature engineering, drop all the columns that are unnecessary\n",
    "data_df = data_df.drop([\"Source Port\", \"Destination Port\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the features and labels in separate dataframes\n",
    "X = data_df.drop(\"Labels\", axis=1)\n",
    "y = data_df[\"Labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data to array\n",
    "data_array = data_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 1\n",
    "for train, test in kfold.split(data_array):\n",
    "    print(f\"Model {model}\")\n",
    "    print(f\"train {train}, test: {test}\")\n",
    "    model += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "knn_model = KNeighborsRegressor(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier(max_depth=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=2, max_depth=2, learning_rate=1, objective=\"binary:logistic\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_model(model, X, y):\n",
    "    accuracy_scores = []\n",
    "    for train, test in kfold.split(X):\n",
    "        X_train, X_test = X.iloc[train], X.iloc[test]\n",
    "        y_train, y_test = y.iloc[train], y.iloc[test]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # evaluate model accuracy\n",
    "        accuracy = model.score(X_test, y_test)\n",
    "        accuracy_scores.append(accuracy)\n",
    "\n",
    "    return accuracy_scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI \n",
    "Use llm for classification. Can only use payload that is words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add labels, 0 for benign, 1 for malicious\n",
    "mirai_cleaned_df[\"Labels\"] = \"positive\"\n",
    "benign_clean_df[\"Labels\"] = \"negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mirai_payloads = mirai_cleaned_df[[\"Payload\", \"Labels\"]]\n",
    "benign_payloads = benign_clean_df[[\"Payload\", \"Labels\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payloads = pd.concat([mirai_payloads, benign_payloads], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use sample\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "payloads = shuffle(payloads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a subset because this takes loooong!\n",
    "X = payloads[\"Payload\"].head(100)\n",
    "y = payloads[\"Labels\"].head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skllm.config import SKLLMConfig\n",
    "\n",
    "SKLLMConfig.set_openai_key(openai_api_key)\n",
    "SKLLMConfig.set_openai_org(\"org-HlcxSARQUphcO0tUGmtJJOpD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: DynamicFewShotGPTClassifier, FewShotGPTClassifier\n",
    "from skllm import ZeroShotGPTClassifier\n",
    "\n",
    "clf = ZeroShotGPTClassifier(openai_model=\"gpt-3.5-turbo\")\n",
    "clf.fit(X, y)\n",
    "labels = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y, labels):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skllm import FewShotGPTClassifier\n",
    "\n",
    "clf = FewShotGPTClassifier(openai_model=\"gpt-3.5-turbo\")\n",
    "clf.fit(X, y)\n",
    "labels = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y, labels):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skllm import DynamicFewShotGPTClassifier\n",
    "\n",
    "clf = DynamicFewShotGPTClassifier(n_examples=3)\n",
    "clf.fit(X, y)\n",
    "labels = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DynamicFewShotGPTClassifier(n_examples=3)\n",
    "clf.fit(X, y)\n",
    "labels = clf.predict(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "Accuracy is the proportion of correct predictions to the total number of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average accuracy score across all folds\n",
    "accuracy_scores = k_fold_model(knn_model, X, y)\n",
    "average_accuracy = sum(accuracy_scores) / k\n",
    "print(\"Average accuracy k-nn:\", average_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average accuracy score across all folds\n",
    "accuracy_scores = k_fold_model(rf_model, X, y)\n",
    "average_accuracy = sum(accuracy_scores) / k\n",
    "print(\"Average accuracy random forest:\", average_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average accuracy score across all folds\n",
    "accuracy_scores = k_fold_model(xgb_model, X, y)\n",
    "average_accuracy = sum(accuracy_scores) / k\n",
    "print(\"Average accuracy XGBoost:\", average_accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision\n",
    "\n",
    "Precision measures the proportion of true positives to the total predicted positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(model, X, y, k, scorer):\n",
    "    # Perform k-fold cross-validation and calculate precision scores\n",
    "    # precision_scores = cross_val_score(model, X, y, cv=k, scoring=scorer)\n",
    "    model_scores = cross_val_score(model, X, y, cv=k)\n",
    "\n",
    "    # Calculate the average score across all folds\n",
    "    average_score = model_scores.mean()\n",
    "\n",
    "    return average_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score\n",
    "\n",
    "# Assuming model is your trained model and X, y are the input features and labels respectively\n",
    "# Set the scoring metric to precision\n",
    "scorer = make_scorer(precision_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: how do we calculate precision for knn?\n",
    "# print(\"Average Precision knn:\", model_evaluation(knn_model, X, y, k, scorer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Average Precision random forest:\", model_evaluation(rf_model, X, y, k, scorer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average Precision xgboost\", model_evaluation(xgb_model, X, y, k, scorer))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall\n",
    "\n",
    "Recall (also known as sensitivity or true positive rate) measures the proportion of true positives to the total actual positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = make_scorer(recall_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average Recall knn:\", model_evaluation(knn_model, X, y, k, scorer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average Recall random forest:\", model_evaluation(rf_model, X, y, k, scorer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average Recall xgboost:\", model_evaluation(xgb_model, X, y, k, scorer))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix\n",
    "\n",
    "A confusion matrix is a table that visualizes the performance of a classification model by showing the counts of true positive, true negative, false positive, and false negative predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(model, X, y):\n",
    "    accuracy_scores = []\n",
    "    for train, test in kfold.split(X):\n",
    "        X_train, X_test = X.iloc[train], X.iloc[test]\n",
    "        y_train, y_test = y.iloc[train], y.iloc[test]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        # Create a heatmap of the confusion matrix\n",
    "        sns.heatmap(cm, annot=True, cmap=\"Blues\")\n",
    "        plt.xlabel(\"Predicted labels\")\n",
    "        plt.ylabel(\"True labels\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(xgb_model, X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC curve\n",
    "\n",
    "Use pandas ai ROC curve"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
